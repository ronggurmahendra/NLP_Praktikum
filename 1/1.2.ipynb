{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praktikum 1.2 Natural Language Processing \n",
    "\n",
    "### Nama : Ronggur Mahendra Widya Putra\n",
    "### NIM : 13519008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "from keras.layers import Dense\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>− Scope 3: Optional scope that includes indire...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Group is not aware of any noise pollution ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Global climate change could exacerbate certain...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Setting an investment horizon is part and parc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Climate change the physical impacts of climate...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Projects with potential limited adverse social...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>We emitted 13.4 million tonnes CO2 of Scope 2 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>We do not provide normalised figures for our C...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>We anticipate that the potential effects of cl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Enhancing our responsible screening criteria N...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  − Scope 3: Optional scope that includes indire...      1\n",
       "1  The Group is not aware of any noise pollution ...      0\n",
       "2  Global climate change could exacerbate certain...      0\n",
       "3  Setting an investment horizon is part and parc...      0\n",
       "4  Climate change the physical impacts of climate...      0\n",
       "5  Projects with potential limited adverse social...      0\n",
       "6  We emitted 13.4 million tonnes CO2 of Scope 2 ...      1\n",
       "7  We do not provide normalised figures for our C...      1\n",
       "8  We anticipate that the potential effects of cl...      0\n",
       "9  Enhancing our responsible screening criteria N...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Train data\n",
    "train_df = pd.read_parquet('./data/train-00000-of-00001-04b49ae22f595095.parquet', engine='pyarrow')\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.764278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label\n",
       "count  1000.000000\n",
       "mean      0.908000\n",
       "std       0.764278\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       1.000000\n",
       "75%       1.250000\n",
       "max       2.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_data  = train_df['text'].to_list()\n",
    "train_df_label = train_df['label'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_data, val_data, train_label, val_label  = train_test_split(train_df_data, train_df_label, test_size=0.2, random_state=230907)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sustainable strategy ‘red lines’ For our susta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Verizon’s environmental, health and safety man...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In 2019, the Company closed a series of transa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In December 2020, the AUC approved the Electri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finally, there is a reputational risk linked t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ecoefficiency Eco-efficiency management provid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Group and its customers are exposed to cli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Both our Board and executive leadership team r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Although it is intended that governments will ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Climate-related risks and opportunities have g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Sustainable strategy ‘red lines’ For our susta...      0\n",
       "1  Verizon’s environmental, health and safety man...      1\n",
       "2  In 2019, the Company closed a series of transa...      1\n",
       "3  In December 2020, the AUC approved the Electri...      0\n",
       "4  Finally, there is a reputational risk linked t...      0\n",
       "5  Ecoefficiency Eco-efficiency management provid...      1\n",
       "6  The Group and its customers are exposed to cli...      0\n",
       "7  Both our Board and executive leadership team r...      1\n",
       "8  Although it is intended that governments will ...      1\n",
       "9  Climate-related risks and opportunities have g...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load test data\n",
    "test_df = pd.read_parquet('./data/test-00000-of-00001-3f9f7af4f5914b8e.parquet', engine='pyarrow')\n",
    "test_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data  = test_df['text'].to_list()\n",
    "test_label = test_df['label'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_label :  800\n",
      "train_label :  800\n",
      "val_label :  200\n",
      "val_label :  200\n",
      "test_data :  320\n",
      "test_label : 320\n"
     ]
    }
   ],
   "source": [
    "print(\"train_label : \", len(train_data))\n",
    "print(\"train_label : \",len(train_label))\n",
    "\n",
    "print(\"val_label : \", len(val_data))\n",
    "print(\"val_label : \",len(val_label))\n",
    "\n",
    "print(\"test_data : \",len(test_data))\n",
    "print(\"test_label :\",len(test_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess & Tokenize\n",
    "MAX_WORDS = 10000\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(texts = train_data)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data)\n",
    "val_sequences = tokenizer.texts_to_sequences(val_data)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data)\n",
    "\n",
    "train_label = np.array(train_label)\n",
    "val_label = np.array(val_label)\n",
    "test_label = np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "train_data_tokenized = pad_sequences(train_sequences, maxlen = 100)\n",
    "val_data_tokenized = pad_sequences(val_sequences, maxlen = 100)\n",
    "test_data_tokenized = pad_sequences(test_sequences, maxlen = 100)\n",
    "\n",
    "# Cast into numpy array \n",
    "train_data_tokenized = np.array(train_data_tokenized)\n",
    "val_data_tokenized = np.array(val_data_tokenized)\n",
    "test_data_tokenized = np.array(test_data_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN/LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "# Hyper parameter sama dengan contoh di slide\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(input_dim = MAX_WORDS, output_dim = 128, input_length = train_data_tokenized.shape[1]))\n",
    "model_rnn.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model_rnn.add(Bidirectional(LSTM(32)))\n",
    "model_rnn.add(Dense(1,activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 128)          1280000   \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 100, 128)          98816     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 64)                41216     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1420097 (5.42 MB)\n",
      "Trainable params: 1420097 (5.42 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "Model Visualize\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "#compile model\n",
    "model_rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_rnn.summary())\n",
    "print(\"\\n\\nModel Visualize\")\n",
    "plot_model(model_rnn, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 24s 360ms/step - loss: 0.2959 - accuracy: 0.4087 - val_loss: 0.5728 - val_accuracy: 0.4350\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 5s 189ms/step - loss: -0.4261 - accuracy: 0.4250 - val_loss: -0.5450 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 5s 191ms/step - loss: -1.9914 - accuracy: 0.5775 - val_loss: -1.4968 - val_accuracy: 0.6500\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 5s 199ms/step - loss: -3.2675 - accuracy: 0.6700 - val_loss: -2.0075 - val_accuracy: 0.6600\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 5s 190ms/step - loss: -3.9906 - accuracy: 0.6800 - val_loss: -2.0686 - val_accuracy: 0.6700\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 5s 193ms/step - loss: -4.6382 - accuracy: 0.7100 - val_loss: -2.1485 - val_accuracy: 0.6600\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 5s 193ms/step - loss: -5.2014 - accuracy: 0.7125 - val_loss: -2.3578 - val_accuracy: 0.6500\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 5s 189ms/step - loss: -5.7742 - accuracy: 0.7237 - val_loss: -2.5058 - val_accuracy: 0.6550\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 5s 194ms/step - loss: -6.2964 - accuracy: 0.7312 - val_loss: -2.6970 - val_accuracy: 0.6550\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 5s 184ms/step - loss: -6.7532 - accuracy: 0.7312 - val_loss: -2.8743 - val_accuracy: 0.6600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1ee0bf715d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "model_rnn.fit(train_data_tokenized, train_label, epochs=10, batch_size=32, validation_data=(val_data_tokenized, val_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 62ms/step - loss: -1.6383 - accuracy: 0.6469\n",
      "loss:  -1.6383483409881592\n",
      "accuracy:  0.6468750238418579\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "\n",
    "loss, acc = model_rnn.evaluate(test_data_tokenized, test_label)\n",
    "print(\"loss: \", loss)\n",
    "print(\"accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n",
      "Text: sustainable strategy for our sustainable strategy range we incorporate a series of proprietary in order to ensure the performing companies from an esg perspective are not eligible for investment \n",
      " Predicted Sentiment: positive\n",
      " Groundtruth: negative\n",
      "\n",
      "\n",
      "Text: environmental health and safety management system provides a framework for identifying and reducing the risks associated with the environments in which we operate regular management system assessments internal and third party compliance and inspections are performed annually at of facilities worldwide the goal of these assessments is to identify and correct site specific issues and to educate and empower facility managers and supervisors to implement corrective actions environment health and safety efforts are and supported by experienced experts around the world that support our operations and facilities \n",
      " Predicted Sentiment: positive\n",
      " Groundtruth: negative\n",
      "\n",
      "\n",
      "Text: in 2019 the company a series of transactions related to the sale of its canadian fossil fuel based electricity generation business a transaction with heartland generation ltd an of energy capital partners included the sale of 10 partly or fully owned natural gas fired and coal fired electricity generation assets located in alberta and british columbia in two other separate transactions the company sold its 50 per cent ownership interest in the cogeneration station to international and its 50 per cent ownership interest in beach power to power generation \n",
      " Predicted Sentiment: positive\n",
      " Groundtruth: negative\n",
      "\n",
      "\n",
      "Text: which would normally come into effect on january 1 2021 for both businesses the rate was to significant distribution rate increases which would be end use customers due to the approach of rate under the mechanism electricity distribution and natural gas distribution the current economic situation in alberta including the faced by some end use customers due to the covid 19 pandemic as rationale to proceed with these interim rates electricity distribution and natural gas distribution are to an application by march 1 2021 the of the rate and collection timelines expected values including costs and anticipated impacts to customers \n",
      " Predicted Sentiment: negative\n",
      " Groundtruth: negative\n",
      "\n",
      "\n",
      "Text: finally there is a reputational risk linked to the possibility that oil companies may be perceived by institutions and the general public as the entities mainly responsible of the climate change this could possibly make eni’s shares less attractive to investment funds and individual investors who assess the risk profile of companies against their environmental and social footprint when making investment decisions \n",
      " Predicted Sentiment: positive\n",
      " Groundtruth: negative\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "prediction = model_rnn.predict(test_data_tokenized[:5])\n",
    "\n",
    "for text, prediction, groundtruth in zip(tokenizer.sequences_to_texts(test_data_tokenized), prediction, test_label[:5]):\n",
    "    sentiment = \"positive\" if prediction > 0.5 else \"negative\"\n",
    "    groundtruth = \"positive\" if groundtruth == 0.5 else \"negative\"\n",
    "    print(f\"Text: {text} \\n Predicted Sentiment: {sentiment}\\n Groundtruth: {groundtruth}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(sentences=train_data, vector_size=128, window = 5, min_count=1, sg=0)\n",
    "\n",
    "word2vec_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((MAX_WORDS, 128))\n",
    "for word,i in tokenizer.word_index.items():\n",
    "    if i < MAX_WORDS:\n",
    "        if word in word2vec_model.wv:\n",
    "            embedding_matrix[i] = word2vec_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "# Hyper parameter sama dengan contoh di slide\n",
    "word2vec_model = Sequential()\n",
    "word2vec_model.add(Embedding(input_dim = MAX_WORDS, output_dim = 128, input_length = train_data_tokenized.shape[1], weights= [embedding_matrix], trainable = True ))\n",
    "word2vec_model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "word2vec_model.add(Bidirectional(LSTM(32)))\n",
    "word2vec_model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 100, 128)          1280000   \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirecti  (None, 100, 128)          98816     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirecti  (None, 64)                41216     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1420097 (5.42 MB)\n",
      "Trainable params: 1420097 (5.42 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "Model Visualize\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "#compile model\n",
    "word2vec_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(word2vec_model.summary())\n",
    "print(\"\\n\\nModel Visualize\")\n",
    "plot_model(word2vec_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 23s 350ms/step - loss: 0.3083 - accuracy: 0.4038 - val_loss: 0.3667 - val_accuracy: 0.4350\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 5s 184ms/step - loss: -0.7952 - accuracy: 0.4363 - val_loss: -0.9207 - val_accuracy: 0.6550\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 5s 187ms/step - loss: -1.9854 - accuracy: 0.6150 - val_loss: -1.0480 - val_accuracy: 0.5700\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 5s 187ms/step - loss: -2.8145 - accuracy: 0.6475 - val_loss: -1.5168 - val_accuracy: 0.6800\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 5s 184ms/step - loss: -3.2401 - accuracy: 0.6700 - val_loss: -2.2172 - val_accuracy: 0.6550\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 4s 172ms/step - loss: -4.3133 - accuracy: 0.6988 - val_loss: -2.6830 - val_accuracy: 0.6750\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 4s 173ms/step - loss: -4.7837 - accuracy: 0.6988 - val_loss: -1.3259 - val_accuracy: 0.6400\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 4s 170ms/step - loss: -5.1003 - accuracy: 0.7163 - val_loss: -2.6164 - val_accuracy: 0.6550\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 4s 171ms/step - loss: -5.9580 - accuracy: 0.7175 - val_loss: -2.8565 - val_accuracy: 0.6500\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 4s 175ms/step - loss: -6.5379 - accuracy: 0.7250 - val_loss: -3.4648 - val_accuracy: 0.6550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1ee0c9d5d50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "word2vec_model.fit(train_data_tokenized, train_label, epochs=10, batch_size=32, validation_data=(val_data_tokenized, val_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 57ms/step - loss: -1.9364 - accuracy: 0.6344\n",
      "loss:  -1.9364475011825562\n",
      "accuracy:  0.6343749761581421\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "\n",
    "loss, acc = word2vec_model.evaluate(test_data_tokenized, test_label)\n",
    "print(\"loss: \", loss)\n",
    "print(\"accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n",
      "Text: sustainable strategy for our sustainable strategy range we incorporate a series of proprietary in order to ensure the performing companies from an esg perspective are not eligible for investment \n",
      " Predicted Sentiment: positive\n",
      " Groundtruth: negative\n",
      "\n",
      "\n",
      "Text: environmental health and safety management system provides a framework for identifying and reducing the risks associated with the environments in which we operate regular management system assessments internal and third party compliance and inspections are performed annually at of facilities worldwide the goal of these assessments is to identify and correct site specific issues and to educate and empower facility managers and supervisors to implement corrective actions environment health and safety efforts are and supported by experienced experts around the world that support our operations and facilities \n",
      " Predicted Sentiment: negative\n",
      " Groundtruth: negative\n",
      "\n",
      "\n",
      "Text: in 2019 the company a series of transactions related to the sale of its canadian fossil fuel based electricity generation business a transaction with heartland generation ltd an of energy capital partners included the sale of 10 partly or fully owned natural gas fired and coal fired electricity generation assets located in alberta and british columbia in two other separate transactions the company sold its 50 per cent ownership interest in the cogeneration station to international and its 50 per cent ownership interest in beach power to power generation \n",
      " Predicted Sentiment: positive\n",
      " Groundtruth: negative\n",
      "\n",
      "\n",
      "Text: which would normally come into effect on january 1 2021 for both businesses the rate was to significant distribution rate increases which would be end use customers due to the approach of rate under the mechanism electricity distribution and natural gas distribution the current economic situation in alberta including the faced by some end use customers due to the covid 19 pandemic as rationale to proceed with these interim rates electricity distribution and natural gas distribution are to an application by march 1 2021 the of the rate and collection timelines expected values including costs and anticipated impacts to customers \n",
      " Predicted Sentiment: positive\n",
      " Groundtruth: negative\n",
      "\n",
      "\n",
      "Text: finally there is a reputational risk linked to the possibility that oil companies may be perceived by institutions and the general public as the entities mainly responsible of the climate change this could possibly make eni’s shares less attractive to investment funds and individual investors who assess the risk profile of companies against their environmental and social footprint when making investment decisions \n",
      " Predicted Sentiment: negative\n",
      " Groundtruth: negative\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "prediction = word2vec_model.predict(test_data_tokenized[:5])\n",
    "\n",
    "for text, prediction, groundtruth in zip(tokenizer.sequences_to_texts(test_data_tokenized), prediction, test_label[:5]):\n",
    "    sentiment = \"positive\" if prediction > 0.5 else \"negative\"\n",
    "    groundtruth = \"positive\" if groundtruth == 0.5 else \"negative\"\n",
    "    print(f\"Text: {text} \\n Predicted Sentiment: {sentiment}\\n Groundtruth: {groundtruth}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, TFDistilBertModel, TFAutoModel, AutoTokenizer\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling1D, Attention, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pretrained language model DistilBERT\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "attention_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# attention_encoder = TFAutoModel.from_pretrained(model_name)\n",
    "attention_model = TFDistilBertModel.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMa  multiple                  66362880  \n",
      " inLayer)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66362880 (253.15 MB)\n",
      "Trainable params: 66362880 (253.15 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "attention_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Exception encountered when calling layer 'distilbert' (type TFDistilBertMainLayer).\n\nToo many inputs.\n\nCall arguments received by layer 'distilbert' (type TFDistilBertMainLayer):\n  • inputs=[\"'replace'\", \"'me'\", \"'by'\", \"'any'\", \"'text'\", \"'you'\", '\"\\'\"', \"'d'\", \"'like'\", \"'.'\"]\n  • attention_mask=None\n  • head_mask=None\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rongg\\kuliah\\Semester 9\\NLP\\Praktikum\\1\\1.2.ipynb Cell 31\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rongg/kuliah/Semester%209/NLP/Praktikum/1/1.2.ipynb#Y114sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mReplace me by any text you\u001b[39m\u001b[39m'\u001b[39m\u001b[39md like.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rongg/kuliah/Semester%209/NLP/Praktikum/1/1.2.ipynb#Y114sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m encoded_input \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rongg/kuliah/Semester%209/NLP/Praktikum/1/1.2.ipynb#Y114sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m output \u001b[39m=\u001b[39m model(encoded_input)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_distilbert.py:545\u001b[0m, in \u001b[0;36mTFDistilBertModel.call\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 545\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistilbert(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    546\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_distilbert.py:412\u001b[0m, in \u001b[0;36mTFDistilBertMainLayer.call\u001b[1;34m(self, inputs, attention_mask, head_mask, training)\u001b[0m\n\u001b[0;32m    410\u001b[0m     attention_mask \u001b[39m=\u001b[39m inputs[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(inputs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m attention_mask\n\u001b[0;32m    411\u001b[0m     head_mask \u001b[39m=\u001b[39m inputs[\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(inputs) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m head_mask\n\u001b[1;32m--> 412\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(inputs) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mToo many inputs.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    413\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    414\u001b[0m     input_ids \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Exception encountered when calling layer 'distilbert' (type TFDistilBertMainLayer).\n\nToo many inputs.\n\nCall arguments received by layer 'distilbert' (type TFDistilBertMainLayer):\n  • inputs=[\"'replace'\", \"'me'\", \"'by'\", \"'any'\", \"'text'\", \"'you'\", '\"\\'\"', \"'d'\", \"'like'\", \"'.'\"]\n  • attention_mask=None\n  • head_mask=None\n  • training=False"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer.tokenize(text)\n",
    "output = model(encoded_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NumPy arrays to lists\n",
    "# train_sequences = train_sequences.tolist()\n",
    "# val_sequences = val_sequences.tolist()\n",
    "# test_sequences = test_sequences.tolist()\n",
    "# train_label = train_label.tolist()\n",
    "# val_label = val_label.tolist()\n",
    "# test_label = test_label.tolist()\n",
    "\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = attention_tokenizer.tokenize(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rongg\\kuliah\\Semester 9\\NLP\\Praktikum\\1\\1.2.ipynb Cell 33\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rongg/kuliah/Semester%209/NLP/Praktikum/1/1.2.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# # # train/finetune the model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rongg/kuliah/Semester%209/NLP/Praktikum/1/1.2.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# train_data_tokenized = pad_sequences(train_sequences, maxlen = 100)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rongg/kuliah/Semester%209/NLP/Praktikum/1/1.2.ipynb#X43sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# val_data_tokenized = pad_sequences(val_sequences, maxlen = 100)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rongg/kuliah/Semester%209/NLP/Praktikum/1/1.2.ipynb#X43sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# test_data_tokenized = pad_sequences(test_sequences, maxlen = 100)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rongg/kuliah/Semester%209/NLP/Praktikum/1/1.2.ipynb#X43sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train_data_tokenized \u001b[39m=\u001b[39m attention_tokenizer\u001b[39m.\u001b[39mtokenize(train_data_tokenized)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rongg/kuliah/Semester%209/NLP/Praktikum/1/1.2.ipynb#X43sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m val_data_tokenized \u001b[39m=\u001b[39m attention_tokenizer\u001b[39m.\u001b[39mtokenize(val_data_tokenized)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rongg/kuliah/Semester%209/NLP/Praktikum/1/1.2.ipynb#X43sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m test_data_tokenized \u001b[39m=\u001b[39m attention_tokenizer\u001b[39m.\u001b[39mtokenize(test_data_tokenized)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:649\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msum\u001b[39m((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenize(token, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mif\u001b[39;00m token \u001b[39mnot\u001b[39;00m \\\n\u001b[0;32m    645\u001b[0m             \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madded_tokens_encoder \u001b[39mand\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_special_tokens \\\n\u001b[0;32m    646\u001b[0m             \u001b[39melse\u001b[39;00m [token] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokenized_text), [])\n\u001b[0;32m    648\u001b[0m added_tokens \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39madded_tokens_encoder\u001b[39m.\u001b[39mkeys()) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_special_tokens\n\u001b[1;32m--> 649\u001b[0m tokenized_text \u001b[39m=\u001b[39m split_on_tokens(added_tokens, text)\n\u001b[0;32m    650\u001b[0m \u001b[39mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:627\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize.<locals>.split_on_tokens\u001b[1;34m(tok_list, text)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msplit_on_tokens\u001b[39m(tok_list, text):\n\u001b[1;32m--> 627\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m text:\n\u001b[0;32m    628\u001b[0m         \u001b[39mreturn\u001b[39;00m []\n\u001b[0;32m    629\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tok_list:\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# # # train/finetune the model\n",
    "# train_data_tokenized = pad_sequences(train_sequences, maxlen = 100)\n",
    "# val_data_tokenized = pad_sequences(val_sequences, maxlen = 100)\n",
    "# test_data_tokenized = pad_sequences(test_sequences, maxlen = 100)\n",
    "train_data_tokenized = attention_tokenizer.tokenize(train_data_tokenized)\n",
    "val_data_tokenized = attention_tokenizer.tokenize(val_data_tokenized)\n",
    "test_data_tokenized = attention_tokenizer.tokenize(test_data_tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_model.fit(train_data_tokenized, train_label, validation_data=(val_data_tokenized, val_label), epochs=3, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2341, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2327, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2315, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2283, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_file5f171465.py\", line 11, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).distilbert, (ag__.ld(inputs),), dict(**ag__.ld(kwargs)), fscope)\n    File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_file0fqi6hhd.py\", line 83, in tf__call\n        embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (ag__.ld(input_ids),), None, fscope)\n    File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_file_fbe7kiy.py\", line 65, in tf__call\n        ag__.if_stmt(ag__.ld(mode) == 'embedding', if_body_1, else_body_1, get_state_1, set_state_1, ('do_return', 'retval_'), 2)\n    File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_file_fbe7kiy.py\", line 40, in if_body_1\n        raise\n    File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_file9r_i8nr8.py\", line 41, in tf___embedding\n        seq_length = ag__.converted_call(ag__.ld(tf).shape, (ag__.ld(input_ids),), None, fscope)[1]\n\n    ValueError: Exception encountered when calling layer 'tf_distil_bert_model' (type TFDistilBertModel).\n    \n    in user code:\n    \n        File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_distilbert.py\", line 545, in call  *\n            outputs = self.distilbert(inputs, **kwargs)\n        File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_file0fqi6hhd.py\", line 83, in tf__call\n            embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (ag__.ld(input_ids),), None, fscope)\n        File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_file_fbe7kiy.py\", line 65, in tf__call\n            ag__.if_stmt(ag__.ld(mode) == 'embedding', if_body_1, else_body_1, get_state_1, set_state_1, ('do_return', 'retval_'), 2)\n        File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_file_fbe7kiy.py\", line 40, in if_body_1\n            raise\n        File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_file9r_i8nr8.py\", line 41, in tf___embedding\n            seq_length = ag__.converted_call(ag__.ld(tf).shape, (ag__.ld(input_ids),), None, fscope)[1]\n    \n        ValueError: Exception encountered when calling layer 'distilbert' (type TFDistilBertMainLayer).\n        \n        in user code:\n        \n            File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_distilbert.py\", line 435, in call  *\n                embedding_output = self.embeddings(input_ids)   # (bs, seq_length, dim)\n            File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_file_fbe7kiy.py\", line 65, in tf__call\n                ag__.if_stmt(ag__.ld(mode) == 'embedding', if_body_1, else_body_1, get_state_1, set_state_1, ('do_return', 'retval_'), 2)\n            File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_file_fbe7kiy.py\", line 40, in if_body_1\n                raise\n            File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_file9r_i8nr8.py\", line 41, in tf___embedding\n                seq_length = ag__.converted_call(ag__.ld(tf).shape, (ag__.ld(input_ids),), None, fscope)[1]\n        \n            ValueError: Exception encountered when calling layer 'embeddings' (type TFEmbeddings).\n            \n            in user code:\n            \n                File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_distilbert.py\", line 115, in call  *\n                    return self._embedding(inputs, training=training)\n                File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_distilbert.py\", line 139, in _embedding  *\n                    seq_length = tf.shape(input_ids)[1]\n            \n                ValueError: slice index 1 of dimension 0 out of bounds. for '{{node tf_distil_bert_model/distilbert/embeddings/strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](tf_distil_bert_model/distilbert/embeddings/Shape, tf_distil_bert_model/distilbert/embeddings/strided_slice/stack, tf_distil_bert_model/distilbert/embeddings/strided_slice/stack_1, tf_distil_bert_model/distilbert/embeddings/strided_slice/stack_2)' with input shapes: [1], [1], [1], [1] and with computed input tensors: input[1] = <1>, input[2] = <2>, input[3] = <1>.\n            \n            \n            Call arguments received by layer 'embeddings' (type TFEmbeddings):\n              • inputs=tf.Tensor(shape=(None,), dtype=string)\n              • mode=embedding\n              • training=False\n        \n        \n        Call arguments received by layer 'distilbert' (type TFDistilBertMainLayer):\n          • inputs=tf.Tensor(shape=(None,), dtype=string)\n          • attention_mask=None\n          • head_mask=None\n          • training=False\n    \n    \n    Call arguments received by layer 'tf_distil_bert_model' (type TFDistilBertModel):\n      • inputs=tf.Tensor(shape=(None,), dtype=string)\n      • kwargs={'training': 'False'}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rongg\\kuliah\\Semester 9\\NLP\\Praktikum\\1\\1.2.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rongg/kuliah/Semester%209/NLP/Praktikum/1/1.2.ipynb#Y115sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m attention_model\u001b[39m.\u001b[39mpredict(encoded_input)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filev26b7qoa.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file5f171465.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m      9\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     10\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 11\u001b[0m outputs \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mdistilbert, (ag__\u001b[39m.\u001b[39mld(inputs),), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(kwargs)), fscope)\n\u001b[0;32m     12\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file0fqi6hhd.py:83\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, inputs, attention_mask, head_mask, training)\u001b[0m\n\u001b[0;32m     81\u001b[0m     head_mask \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mnum_hidden_layers\n\u001b[0;32m     82\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mld(head_mask) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, if_body_3, else_body_3, get_state_3, set_state_3, (\u001b[39m'\u001b[39m\u001b[39mhead_mask\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39m1\u001b[39m)\n\u001b[1;32m---> 83\u001b[0m embedding_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39membeddings, (ag__\u001b[39m.\u001b[39mld(input_ids),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     84\u001b[0m tfmr_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mtransformer, ([ag__\u001b[39m.\u001b[39mld(embedding_output), ag__\u001b[39m.\u001b[39mld(attention_mask), ag__\u001b[39m.\u001b[39mld(head_mask)],), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(training)), fscope)\n\u001b[0;32m     85\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file_fbe7kiy.py:65\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, inputs, mode, training)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[39mraise\u001b[39;00m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mValueError\u001b[39;00m), (ag__\u001b[39m.\u001b[39mconverted_call(\u001b[39m'\u001b[39m\u001b[39mmode \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is not valid.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat, (ag__\u001b[39m.\u001b[39mld(mode),), \u001b[39mNone\u001b[39;00m, fscope),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     64\u001b[0m     ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mld(mode) \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m'\u001b[39m, if_body, else_body, get_state, set_state, (\u001b[39m'\u001b[39m\u001b[39mdo_return\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mretval_\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m2\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mld(mode) \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m'\u001b[39m, if_body_1, else_body_1, get_state_1, set_state_1, (\u001b[39m'\u001b[39m\u001b[39mdo_return\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mretval_\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m2\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[39mreturn\u001b[39;00m fscope\u001b[39m.\u001b[39mret(retval_, do_return)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file_fbe7kiy.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.if_body_1\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_embedding, (ag__\u001b[39m.\u001b[39mld(inputs),), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(training)), fscope)\n\u001b[0;32m     38\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file9r_i8nr8.py:41\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___embedding\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m     39\u001b[0m input_ids \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     40\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mnot_(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39misinstance\u001b[39m), (ag__\u001b[39m.\u001b[39mld(inputs), (ag__\u001b[39m.\u001b[39mld(\u001b[39mtuple\u001b[39m), ag__\u001b[39m.\u001b[39mld(\u001b[39mlist\u001b[39m))), \u001b[39mNone\u001b[39;00m, fscope)), if_body, else_body, get_state, set_state, (\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mposition_ids\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m2\u001b[39m)\n\u001b[1;32m---> 41\u001b[0m seq_length \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mshape, (ag__\u001b[39m.\u001b[39mld(input_ids),), \u001b[39mNone\u001b[39;00m, fscope)[\u001b[39m1\u001b[39m]\n\u001b[0;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_1\u001b[39m():\n\u001b[0;32m     44\u001b[0m     \u001b[39mreturn\u001b[39;00m (position_ids,)\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2341, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2327, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2315, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2283, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_file5f171465.py\", line 11, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).distilbert, (ag__.ld(inputs),), dict(**ag__.ld(kwargs)), fscope)\n    File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_file0fqi6hhd.py\", line 83, in tf__call\n        embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (ag__.ld(input_ids),), None, fscope)\n    File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_file_fbe7kiy.py\", line 65, in tf__call\n        ag__.if_stmt(ag__.ld(mode) == 'embedding', if_body_1, else_body_1, get_state_1, set_state_1, ('do_return', 'retval_'), 2)\n    File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_file_fbe7kiy.py\", line 40, in if_body_1\n        raise\n    File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_file9r_i8nr8.py\", line 41, in tf___embedding\n        seq_length = ag__.converted_call(ag__.ld(tf).shape, (ag__.ld(input_ids),), None, fscope)[1]\n\n    ValueError: Exception encountered when calling layer 'tf_distil_bert_model' (type TFDistilBertModel).\n    \n    in user code:\n    \n        File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_distilbert.py\", line 545, in call  *\n            outputs = self.distilbert(inputs, **kwargs)\n        File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_file0fqi6hhd.py\", line 83, in tf__call\n            embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (ag__.ld(input_ids),), None, fscope)\n        File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_file_fbe7kiy.py\", line 65, in tf__call\n            ag__.if_stmt(ag__.ld(mode) == 'embedding', if_body_1, else_body_1, get_state_1, set_state_1, ('do_return', 'retval_'), 2)\n        File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_file_fbe7kiy.py\", line 40, in if_body_1\n            raise\n        File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_file9r_i8nr8.py\", line 41, in tf___embedding\n            seq_length = ag__.converted_call(ag__.ld(tf).shape, (ag__.ld(input_ids),), None, fscope)[1]\n    \n        ValueError: Exception encountered when calling layer 'distilbert' (type TFDistilBertMainLayer).\n        \n        in user code:\n        \n            File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_distilbert.py\", line 435, in call  *\n                embedding_output = self.embeddings(input_ids)   # (bs, seq_length, dim)\n            File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_file_fbe7kiy.py\", line 65, in tf__call\n                ag__.if_stmt(ag__.ld(mode) == 'embedding', if_body_1, else_body_1, get_state_1, set_state_1, ('do_return', 'retval_'), 2)\n            File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_file_fbe7kiy.py\", line 40, in if_body_1\n                raise\n            File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_file9r_i8nr8.py\", line 41, in tf___embedding\n                seq_length = ag__.converted_call(ag__.ld(tf).shape, (ag__.ld(input_ids),), None, fscope)[1]\n        \n            ValueError: Exception encountered when calling layer 'embeddings' (type TFEmbeddings).\n            \n            in user code:\n            \n                File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_distilbert.py\", line 115, in call  *\n                    return self._embedding(inputs, training=training)\n                File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_distilbert.py\", line 139, in _embedding  *\n                    seq_length = tf.shape(input_ids)[1]\n            \n                ValueError: slice index 1 of dimension 0 out of bounds. for '{{node tf_distil_bert_model/distilbert/embeddings/strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](tf_distil_bert_model/distilbert/embeddings/Shape, tf_distil_bert_model/distilbert/embeddings/strided_slice/stack, tf_distil_bert_model/distilbert/embeddings/strided_slice/stack_1, tf_distil_bert_model/distilbert/embeddings/strided_slice/stack_2)' with input shapes: [1], [1], [1], [1] and with computed input tensors: input[1] = <1>, input[2] = <2>, input[3] = <1>.\n            \n            \n            Call arguments received by layer 'embeddings' (type TFEmbeddings):\n              • inputs=tf.Tensor(shape=(None,), dtype=string)\n              • mode=embedding\n              • training=False\n        \n        \n        Call arguments received by layer 'distilbert' (type TFDistilBertMainLayer):\n          • inputs=tf.Tensor(shape=(None,), dtype=string)\n          • attention_mask=None\n          • head_mask=None\n          • training=False\n    \n    \n    Call arguments received by layer 'tf_distil_bert_model' (type TFDistilBertModel):\n      • inputs=tf.Tensor(shape=(None,), dtype=string)\n      • kwargs={'training': 'False'}\n"
     ]
    }
   ],
   "source": [
    "attention_model.predict(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "## Performance\n",
    "\n",
    "- LSTM MODEL\n",
    "\n",
    "    Training Accucacy : 0.71\n",
    "\n",
    "    Test Accucacy : 0.70\n",
    "\n",
    "- Word2Vec Embedding\n",
    "\n",
    "    Training Accucacy : 0.70\n",
    "\n",
    "    Test Accucacy : 0.64\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/distilbert-base-uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
