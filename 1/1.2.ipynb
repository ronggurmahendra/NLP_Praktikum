{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praktikum 1.2 Natural Language Processing \n",
    "\n",
    "### Nama : Ronggur Mahendra Widya Putra\n",
    "### NIM : 13519008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "from keras.layers import Dense\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>− Scope 3: Optional scope that includes indire...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Group is not aware of any noise pollution ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Global climate change could exacerbate certain...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Setting an investment horizon is part and parc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Climate change the physical impacts of climate...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Projects with potential limited adverse social...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>We emitted 13.4 million tonnes CO2 of Scope 2 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>We do not provide normalised figures for our C...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>We anticipate that the potential effects of cl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Enhancing our responsible screening criteria N...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  − Scope 3: Optional scope that includes indire...      1\n",
       "1  The Group is not aware of any noise pollution ...      0\n",
       "2  Global climate change could exacerbate certain...      0\n",
       "3  Setting an investment horizon is part and parc...      0\n",
       "4  Climate change the physical impacts of climate...      0\n",
       "5  Projects with potential limited adverse social...      0\n",
       "6  We emitted 13.4 million tonnes CO2 of Scope 2 ...      1\n",
       "7  We do not provide normalised figures for our C...      1\n",
       "8  We anticipate that the potential effects of cl...      0\n",
       "9  Enhancing our responsible screening criteria N...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Train data\n",
    "train_df = pd.read_parquet('./data/train-00000-of-00001-04b49ae22f595095.parquet', engine='pyarrow')\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.764278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label\n",
       "count  1000.000000\n",
       "mean      0.908000\n",
       "std       0.764278\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       1.000000\n",
       "75%       1.250000\n",
       "max       2.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_data  = train_df['text'].to_list()\n",
    "train_df_label = train_df['label'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_data, val_data, train_label, val_label  = train_test_split(train_df_data, train_df_label, test_size=0.2, random_state=230907)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sustainable strategy ‘red lines’ For our susta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Verizon’s environmental, health and safety man...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In 2019, the Company closed a series of transa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In December 2020, the AUC approved the Electri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finally, there is a reputational risk linked t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ecoefficiency Eco-efficiency management provid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Group and its customers are exposed to cli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Both our Board and executive leadership team r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Although it is intended that governments will ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Climate-related risks and opportunities have g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Sustainable strategy ‘red lines’ For our susta...      0\n",
       "1  Verizon’s environmental, health and safety man...      1\n",
       "2  In 2019, the Company closed a series of transa...      1\n",
       "3  In December 2020, the AUC approved the Electri...      0\n",
       "4  Finally, there is a reputational risk linked t...      0\n",
       "5  Ecoefficiency Eco-efficiency management provid...      1\n",
       "6  The Group and its customers are exposed to cli...      0\n",
       "7  Both our Board and executive leadership team r...      1\n",
       "8  Although it is intended that governments will ...      1\n",
       "9  Climate-related risks and opportunities have g...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load test data\n",
    "test_df = pd.read_parquet('./data/test-00000-of-00001-3f9f7af4f5914b8e.parquet', engine='pyarrow')\n",
    "test_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data  = test_df['text'].to_list()\n",
    "test_label = test_df['label'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_label :  800\n",
      "train_label :  800\n",
      "val_label :  200\n",
      "val_label :  200\n",
      "test_data :  320\n",
      "test_label : 320\n"
     ]
    }
   ],
   "source": [
    "print(\"train_label : \", len(train_data))\n",
    "print(\"train_label : \",len(train_label))\n",
    "\n",
    "print(\"val_label : \", len(val_data))\n",
    "print(\"val_label : \",len(val_label))\n",
    "\n",
    "print(\"test_data : \",len(test_data))\n",
    "print(\"test_label :\",len(test_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess & Tokenize\n",
    "MAX_WORDS = 10000\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(texts = train_data)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data)\n",
    "val_sequences = tokenizer.texts_to_sequences(val_data)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data)\n",
    "\n",
    "train_label = np.array(train_label)\n",
    "val_label = np.array(val_label)\n",
    "test_label = np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "train_data_tokenized = pad_sequences(train_sequences, maxlen = 100)\n",
    "val_data_tokenized = pad_sequences(val_sequences, maxlen = 100)\n",
    "test_data_tokenized = pad_sequences(test_sequences, maxlen = 100)\n",
    "\n",
    "# Cast into numpy array \n",
    "train_data_tokenized = np.array(train_data_tokenized)\n",
    "val_data_tokenized = np.array(val_data_tokenized)\n",
    "test_data_tokenized = np.array(test_data_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN/LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "# Hyper parameter sama dengan contoh di slide\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(input_dim = MAX_WORDS, output_dim = 128, input_length = train_data_tokenized.shape[1]))\n",
    "model_rnn.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model_rnn.add(Bidirectional(LSTM(32)))\n",
    "model_rnn.add(Dense(1,activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 128)          1280000   \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 100, 128)          98816     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 64)                41216     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1420097 (5.42 MB)\n",
      "Trainable params: 1420097 (5.42 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "Model Visualize\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "#compile model\n",
    "model_rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_rnn.summary())\n",
    "print(\"\\n\\nModel Visualize\")\n",
    "plot_model(model_rnn, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 13s 232ms/step - loss: 0.3838 - accuracy: 0.4000 - val_loss: 0.4681 - val_accuracy: 0.4350\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 4s 157ms/step - loss: -0.5730 - accuracy: 0.4013 - val_loss: -0.6232 - val_accuracy: 0.4400\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 4s 150ms/step - loss: -1.6813 - accuracy: 0.4013 - val_loss: -0.6545 - val_accuracy: 0.4400\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 4s 142ms/step - loss: -2.2026 - accuracy: 0.4050 - val_loss: -1.2873 - val_accuracy: 0.4400\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 4s 158ms/step - loss: -2.9483 - accuracy: 0.5562 - val_loss: -1.5787 - val_accuracy: 0.6350\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 4s 155ms/step - loss: -3.7860 - accuracy: 0.6637 - val_loss: -1.8579 - val_accuracy: 0.6400\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 4s 150ms/step - loss: -4.4043 - accuracy: 0.6938 - val_loss: -2.0847 - val_accuracy: 0.6450\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 4s 152ms/step - loss: -4.9192 - accuracy: 0.6988 - val_loss: -2.3335 - val_accuracy: 0.6500\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 4s 155ms/step - loss: -5.4546 - accuracy: 0.6925 - val_loss: -2.4008 - val_accuracy: 0.6250\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 4s 141ms/step - loss: -5.7178 - accuracy: 0.6413 - val_loss: -2.4535 - val_accuracy: 0.5600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23c7c9202d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "model_rnn.fit(train_data_tokenized, train_label, epochs=10, batch_size=32, validation_data=(val_data_tokenized, val_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/10 [==>...........................] - ETA: 0s - loss: -0.3352 - accuracy: 0.6875"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 40ms/step - loss: -1.3483 - accuracy: 0.5906\n",
      "loss:  -1.3482943773269653\n",
      "accuracy:  0.590624988079071\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "\n",
    "loss, acc = model_rnn.evaluate(test_data_tokenized, test_label)\n",
    "print(\"loss: \", loss)\n",
    "print(\"accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "Text: sustainable strategy for our sustainable strategy range we incorporate a series of proprietary in order to ensure the performing companies from an esg perspective are not eligible for investment \n",
      " Predicted Sentiment: positive\n",
      " Groundtruth: negative\n",
      "\n",
      "\n",
      "Text: environmental health and safety management system provides a framework for identifying and reducing the risks associated with the environments in which we operate regular management system assessments internal and third party compliance and inspections are performed annually at of facilities worldwide the goal of these assessments is to identify and correct site specific issues and to educate and empower facility managers and supervisors to implement corrective actions environment health and safety efforts are and supported by experienced experts around the world that support our operations and facilities \n",
      " Predicted Sentiment: positive\n",
      " Groundtruth: negative\n",
      "\n",
      "\n",
      "Text: in 2019 the company a series of transactions related to the sale of its canadian fossil fuel based electricity generation business a transaction with heartland generation ltd an of energy capital partners included the sale of 10 partly or fully owned natural gas fired and coal fired electricity generation assets located in alberta and british columbia in two other separate transactions the company sold its 50 per cent ownership interest in the cogeneration station to international and its 50 per cent ownership interest in beach power to power generation \n",
      " Predicted Sentiment: positive\n",
      " Groundtruth: negative\n",
      "\n",
      "\n",
      "Text: which would normally come into effect on january 1 2021 for both businesses the rate was to significant distribution rate increases which would be end use customers due to the approach of rate under the mechanism electricity distribution and natural gas distribution the current economic situation in alberta including the faced by some end use customers due to the covid 19 pandemic as rationale to proceed with these interim rates electricity distribution and natural gas distribution are to an application by march 1 2021 the of the rate and collection timelines expected values including costs and anticipated impacts to customers \n",
      " Predicted Sentiment: negative\n",
      " Groundtruth: negative\n",
      "\n",
      "\n",
      "Text: finally there is a reputational risk linked to the possibility that oil companies may be perceived by institutions and the general public as the entities mainly responsible of the climate change this could possibly make eni’s shares less attractive to investment funds and individual investors who assess the risk profile of companies against their environmental and social footprint when making investment decisions \n",
      " Predicted Sentiment: positive\n",
      " Groundtruth: negative\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "prediction = model_rnn.predict(test_data_tokenized[:5])\n",
    "\n",
    "for text, prediction, groundtruth in zip(tokenizer.sequences_to_texts(test_data_tokenized), prediction, test_label[:5]):\n",
    "    sentiment = \"positive\" if prediction > 0.5 else \"negative\"\n",
    "    groundtruth = \"positive\" if groundtruth == 0.5 else \"negative\"\n",
    "    print(f\"Text: {text} \\n Predicted Sentiment: {sentiment}\\n Groundtruth: {groundtruth}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(sentences=train_data, vector_size=128, window = 5, min_count=1, sg=0)\n",
    "\n",
    "word2vec_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((MAX_WORDS, 128))\n",
    "for word,i in tokenizer.word_index.items():\n",
    "    if i < MAX_WORDS:\n",
    "        if word in word2vec_model.wv:\n",
    "            embedding_matrix[i] = word2vec_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "# Hyper parameter sama dengan contoh di slide\n",
    "word2vec_model = Sequential()\n",
    "word2vec_model.add(Embedding(input_dim = MAX_WORDS, output_dim = 128, input_length = train_data_tokenized.shape[1], weights= [embedding_matrix], trainable = True ))\n",
    "word2vec_model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "word2vec_model.add(Bidirectional(LSTM(32)))\n",
    "word2vec_model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 100, 128)          1280000   \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirecti  (None, 100, 128)          98816     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirecti  (None, 64)                41216     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1420097 (5.42 MB)\n",
      "Trainable params: 1420097 (5.42 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "Model Visualize\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "#compile model\n",
    "word2vec_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(word2vec_model.summary())\n",
    "print(\"\\n\\nModel Visualize\")\n",
    "plot_model(word2vec_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 14s 225ms/step - loss: 0.4480 - accuracy: 0.4062 - val_loss: 0.4463 - val_accuracy: 0.4350\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 4s 152ms/step - loss: -0.3935 - accuracy: 0.4187 - val_loss: -0.5603 - val_accuracy: 0.5100\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 4s 152ms/step - loss: -1.8381 - accuracy: 0.5738 - val_loss: -1.3083 - val_accuracy: 0.6600\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 4s 154ms/step - loss: -2.6364 - accuracy: 0.6700 - val_loss: -1.4094 - val_accuracy: 0.6600\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 4s 150ms/step - loss: -3.1499 - accuracy: 0.7088 - val_loss: -1.6768 - val_accuracy: 0.6550\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 4s 146ms/step - loss: -3.6865 - accuracy: 0.7225 - val_loss: -1.9882 - val_accuracy: 0.6700\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 4s 164ms/step - loss: -4.2057 - accuracy: 0.7225 - val_loss: -2.0621 - val_accuracy: 0.6500\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 4s 162ms/step - loss: -4.7497 - accuracy: 0.7138 - val_loss: -2.1871 - val_accuracy: 0.6450\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 4s 162ms/step - loss: -5.2813 - accuracy: 0.6762 - val_loss: -2.1860 - val_accuracy: 0.6350\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 4s 147ms/step - loss: -5.9835 - accuracy: 0.7200 - val_loss: -2.4815 - val_accuracy: 0.6350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23c02148250>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "word2vec_model.fit(train_data_tokenized, train_label, epochs=10, batch_size=32, validation_data=(val_data_tokenized, val_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 53ms/step - loss: -1.7241 - accuracy: 0.6844\n",
      "loss:  -1.724050760269165\n",
      "accuracy:  0.684374988079071\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "\n",
    "loss, acc = word2vec_model.evaluate(test_data_tokenized, test_label)\n",
    "print(\"loss: \", loss)\n",
    "print(\"accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "Text: sustainable strategy for our sustainable strategy range we incorporate a series of proprietary in order to ensure the performing companies from an esg perspective are not eligible for investment \n",
      " Predicted Sentiment: positive\n",
      " Groundtruth: negative\n",
      "\n",
      "\n",
      "Text: environmental health and safety management system provides a framework for identifying and reducing the risks associated with the environments in which we operate regular management system assessments internal and third party compliance and inspections are performed annually at of facilities worldwide the goal of these assessments is to identify and correct site specific issues and to educate and empower facility managers and supervisors to implement corrective actions environment health and safety efforts are and supported by experienced experts around the world that support our operations and facilities \n",
      " Predicted Sentiment: positive\n",
      " Groundtruth: negative\n",
      "\n",
      "\n",
      "Text: in 2019 the company a series of transactions related to the sale of its canadian fossil fuel based electricity generation business a transaction with heartland generation ltd an of energy capital partners included the sale of 10 partly or fully owned natural gas fired and coal fired electricity generation assets located in alberta and british columbia in two other separate transactions the company sold its 50 per cent ownership interest in the cogeneration station to international and its 50 per cent ownership interest in beach power to power generation \n",
      " Predicted Sentiment: positive\n",
      " Groundtruth: negative\n",
      "\n",
      "\n",
      "Text: which would normally come into effect on january 1 2021 for both businesses the rate was to significant distribution rate increases which would be end use customers due to the approach of rate under the mechanism electricity distribution and natural gas distribution the current economic situation in alberta including the faced by some end use customers due to the covid 19 pandemic as rationale to proceed with these interim rates electricity distribution and natural gas distribution are to an application by march 1 2021 the of the rate and collection timelines expected values including costs and anticipated impacts to customers \n",
      " Predicted Sentiment: negative\n",
      " Groundtruth: negative\n",
      "\n",
      "\n",
      "Text: finally there is a reputational risk linked to the possibility that oil companies may be perceived by institutions and the general public as the entities mainly responsible of the climate change this could possibly make eni’s shares less attractive to investment funds and individual investors who assess the risk profile of companies against their environmental and social footprint when making investment decisions \n",
      " Predicted Sentiment: negative\n",
      " Groundtruth: negative\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "prediction = word2vec_model.predict(test_data_tokenized[:5])\n",
    "\n",
    "for text, prediction, groundtruth in zip(tokenizer.sequences_to_texts(test_data_tokenized), prediction, test_label[:5]):\n",
    "    sentiment = \"positive\" if prediction > 0.5 else \"negative\"\n",
    "    groundtruth = \"positive\" if groundtruth == 0.5 else \"negative\"\n",
    "    print(f\"Text: {text} \\n Predicted Sentiment: {sentiment}\\n Groundtruth: {groundtruth}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import DistilBertTokenizer, TFDistilBertModel, TFAutoModel, AutoTokenizer\n",
    "import transformers\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling1D, Attention, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_label :  800\n",
      "train_label :  800\n",
      "val_label :  200\n",
      "val_label :  200\n",
      "test_data :  320\n",
      "test_label : 320\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Data\n",
    "# print(\"train_label : \", len(train_data))\n",
    "# print(\"train_label : \",len(train_label))\n",
    "\n",
    "# print(\"val_label : \", len(val_data))\n",
    "# print(\"val_label : \",len(val_label))\n",
    "\n",
    "# print(\"test_data : \",len(test_data))\n",
    "# print(\"test_label :\",len(test_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled train data length: 8\n",
      "Sampled train label length: 8\n",
      "Sampled val data length: 2\n",
      "Sampled val label length: 2\n"
     ]
    }
   ],
   "source": [
    "# # preprocess\n",
    "# sample_fraction = 0.01\n",
    "# num_samples = int(len(train_data) * sample_fraction)\n",
    "# sampled_indices = random.sample(range(len(train_data)), num_samples)\n",
    "# sampled_train_data = [train_data[i] for i in sampled_indices]\n",
    "# sampled_train_label = [train_label[i] for i in sampled_indices]\n",
    "# print(\"Sampled train data length:\", len(sampled_train_data))\n",
    "# print(\"Sampled train label length:\", len(sampled_train_label))\n",
    "\n",
    "# # preprocess\n",
    "# sample_fraction = 0.01\n",
    "# num_samples = int(len(val_data) * sample_fraction)\n",
    "# sampled_indices = random.sample(range(len(val_data)), num_samples)\n",
    "# sampled_val_data = [val_data[i] for i in sampled_indices]\n",
    "# sampled_val_data = [val_data[i] for i in sampled_indices]\n",
    "# print(\"Sampled val data length:\", len(sampled_val_data))\n",
    "# print(\"Sampled val label length:\", len(sampled_val_data))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "# training_dataset = datasets.DatasetDict({\"train\":sampled_train_data,\"test\":sampled_val_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = pd.read_parquet('./data/train-00000-of-00001-04b49ae22f595095.parquet', engine='pyarrow')\n",
    "dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pretrained language model DistilBERT\n",
    "model_name = \"bert-base-uncased\"\n",
    "attention_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Values in `DatasetDict` should be of type `Dataset` but got type '<class 'list'>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rongg\\kuliah\\Semester 9\\NLP\\Praktikum\\1\\1.2.ipynb Cell 34\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rongg/kuliah/Semester%209/NLP/Praktikum/1/1.2.ipynb#X66sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize_func\u001b[39m(examples):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rongg/kuliah/Semester%209/NLP/Praktikum/1/1.2.ipynb#X66sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer(examples[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m], padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m'\u001b[39m, trunction \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rongg/kuliah/Semester%209/NLP/Praktikum/1/1.2.ipynb#X66sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m training_dataset_tokenized \u001b[39m=\u001b[39m training_dataset\u001b[39m.\u001b[39mmap(tokenize_func,batched \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\dataset_dict.py:849\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[0;32m    749\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\n\u001b[0;32m    750\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    751\u001b[0m     function: Optional[Callable] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    767\u001b[0m     desc: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    768\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    769\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Apply a function to all the elements in the table (individually or in batches)\u001b[39;00m\n\u001b[0;32m    770\u001b[0m \u001b[39m    and update the table (if function does updated examples).\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[39m    The transformation is applied to all the datasets of the dataset dictionary.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    847\u001b[0m \u001b[39m    ```\u001b[39;00m\n\u001b[0;32m    848\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 849\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_values_type()\n\u001b[0;32m    850\u001b[0m     \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    851\u001b[0m         cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\dataset_dict.py:47\u001b[0m, in \u001b[0;36mDatasetDict._check_values_type\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues():\n\u001b[0;32m     46\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(dataset, Dataset):\n\u001b[1;32m---> 47\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValues in `DatasetDict` should be of type `Dataset` but got type \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(dataset)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Values in `DatasetDict` should be of type `Dataset` but got type '<class 'list'>'"
     ]
    }
   ],
   "source": [
    "def tokenize_func(examples):\n",
    "    return tokenizer(examples['content'], padding='max_length', trunction = True)\n",
    "\n",
    "training_dataset_tokenized = training_dataset.map(tokenize_func,batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 433/433 [00:00<?, ?B/s]\n",
      " 14%|█████████▌                                                        | 63404032/440473133 [07:38<07:22, 851893.98B/s]"
     ]
    }
   ],
   "source": [
    "# Define Model\n",
    "attention_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMa  multiple                  66362880  \n",
      " inLayer)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66362880 (253.15 MB)\n",
      "Trainable params: 66362880 (253.15 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "attention_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NumPy arrays to lists\n",
    "# train_sequences = train_sequences.tolist()\n",
    "# val_sequences = val_sequences.tolist()\n",
    "# test_sequences = test_sequences.tolist()\n",
    "# train_label = train_label.tolist()\n",
    "# val_label = val_label.tolist()\n",
    "# test_label = test_label.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rongg\\kuliah\\Semester 9\\NLP\\Praktikum\\1\\1.2.ipynb Cell 35\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rongg/kuliah/Semester%209/NLP/Praktikum/1/1.2.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# # # train/finetune the model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rongg/kuliah/Semester%209/NLP/Praktikum/1/1.2.ipynb#X46sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# train_data_tokenized = pad_sequences(train_sequences, maxlen = 100)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rongg/kuliah/Semester%209/NLP/Praktikum/1/1.2.ipynb#X46sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# val_data_tokenized = pad_sequences(val_sequences, maxlen = 100)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rongg/kuliah/Semester%209/NLP/Praktikum/1/1.2.ipynb#X46sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# test_data_tokenized = pad_sequences(test_sequences, maxlen = 100)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rongg/kuliah/Semester%209/NLP/Praktikum/1/1.2.ipynb#X46sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train_data_tokenized \u001b[39m=\u001b[39m attention_tokenizer\u001b[39m.\u001b[39mtokenize(train_data_tokenized)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rongg/kuliah/Semester%209/NLP/Praktikum/1/1.2.ipynb#X46sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m val_data_tokenized \u001b[39m=\u001b[39m attention_tokenizer\u001b[39m.\u001b[39mtokenize(val_data_tokenized)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rongg/kuliah/Semester%209/NLP/Praktikum/1/1.2.ipynb#X46sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m test_data_tokenized \u001b[39m=\u001b[39m attention_tokenizer\u001b[39m.\u001b[39mtokenize(test_data_tokenized)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:649\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msum\u001b[39m((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenize(token, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mif\u001b[39;00m token \u001b[39mnot\u001b[39;00m \\\n\u001b[0;32m    645\u001b[0m             \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madded_tokens_encoder \u001b[39mand\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_special_tokens \\\n\u001b[0;32m    646\u001b[0m             \u001b[39melse\u001b[39;00m [token] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokenized_text), [])\n\u001b[0;32m    648\u001b[0m added_tokens \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39madded_tokens_encoder\u001b[39m.\u001b[39mkeys()) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_special_tokens\n\u001b[1;32m--> 649\u001b[0m tokenized_text \u001b[39m=\u001b[39m split_on_tokens(added_tokens, text)\n\u001b[0;32m    650\u001b[0m \u001b[39mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:627\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize.<locals>.split_on_tokens\u001b[1;34m(tok_list, text)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msplit_on_tokens\u001b[39m(tok_list, text):\n\u001b[1;32m--> 627\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m text:\n\u001b[0;32m    628\u001b[0m         \u001b[39mreturn\u001b[39;00m []\n\u001b[0;32m    629\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tok_list:\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# # # train/finetune the model\n",
    "# train_data_tokenized = pad_sequences(train_sequences, maxlen = 100)\n",
    "# val_data_tokenized = pad_sequences(val_sequences, maxlen = 100)\n",
    "# test_data_tokenized = pad_sequences(test_sequences, maxlen = 100)\n",
    "train_data_tokenized = attention_tokenizer.tokenize(train_data_tokenized)\n",
    "val_data_tokenized = attention_tokenizer.tokenize(val_data_tokenized)\n",
    "test_data_tokenized = attention_tokenizer.tokenize(test_data_tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rongg\\kuliah\\Semester 9\\NLP\\Praktikum\\1\\1.2.ipynb Cell 36\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rongg/kuliah/Semester%209/NLP/Praktikum/1/1.2.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m attention_model\u001b[39m.\u001b[39mfit(train_data_tokenized, train_label, validation_data\u001b[39m=\u001b[39m(val_data_tokenized, val_label), epochs\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3875\u001b[0m, in \u001b[0;36mModel._assert_compile_was_called\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3869\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_assert_compile_was_called\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   3870\u001b[0m     \u001b[39m# Checks whether `compile` has been called. If it has been called,\u001b[39;00m\n\u001b[0;32m   3871\u001b[0m     \u001b[39m# then the optimizer is set. This is different from whether the\u001b[39;00m\n\u001b[0;32m   3872\u001b[0m     \u001b[39m# model is compiled\u001b[39;00m\n\u001b[0;32m   3873\u001b[0m     \u001b[39m# (i.e. whether the model is built and its inputs/outputs are set).\u001b[39;00m\n\u001b[0;32m   3874\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_compiled:\n\u001b[1;32m-> 3875\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   3876\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou must compile your model before \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3877\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtraining/testing. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3878\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUse `model.compile(optimizer, loss)`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3879\u001b[0m         )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "attention_model.fit(train_data_tokenized, train_label, validation_data=(val_data_tokenized, val_label), epochs=3, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2341, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2327, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2315, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2283, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_filei_if7x8f.py\", line 11, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).distilbert, (ag__.ld(inputs),), dict(**ag__.ld(kwargs)), fscope)\n    File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_fileexntdafw.py\", line 83, in tf__call\n        embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (ag__.ld(input_ids),), None, fscope)\n    File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_filevom8hoir.py\", line 65, in tf__call\n        ag__.if_stmt(ag__.ld(mode) == 'embedding', if_body_1, else_body_1, get_state_1, set_state_1, ('do_return', 'retval_'), 2)\n    File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_filevom8hoir.py\", line 40, in if_body_1\n        raise\n    File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_fileicn1_bl6.py\", line 41, in tf___embedding\n        seq_length = ag__.converted_call(ag__.ld(tf).shape, (ag__.ld(input_ids),), None, fscope)[1]\n\n    ValueError: Exception encountered when calling layer 'tf_distil_bert_model' (type TFDistilBertModel).\n    \n    in user code:\n    \n        File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_distilbert.py\", line 545, in call  *\n            outputs = self.distilbert(inputs, **kwargs)\n        File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_fileexntdafw.py\", line 83, in tf__call\n            embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (ag__.ld(input_ids),), None, fscope)\n        File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_filevom8hoir.py\", line 65, in tf__call\n            ag__.if_stmt(ag__.ld(mode) == 'embedding', if_body_1, else_body_1, get_state_1, set_state_1, ('do_return', 'retval_'), 2)\n        File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_filevom8hoir.py\", line 40, in if_body_1\n            raise\n        File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_fileicn1_bl6.py\", line 41, in tf___embedding\n            seq_length = ag__.converted_call(ag__.ld(tf).shape, (ag__.ld(input_ids),), None, fscope)[1]\n    \n        ValueError: Exception encountered when calling layer 'distilbert' (type TFDistilBertMainLayer).\n        \n        in user code:\n        \n            File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_distilbert.py\", line 435, in call  *\n                embedding_output = self.embeddings(input_ids)   # (bs, seq_length, dim)\n            File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_filevom8hoir.py\", line 65, in tf__call\n                ag__.if_stmt(ag__.ld(mode) == 'embedding', if_body_1, else_body_1, get_state_1, set_state_1, ('do_return', 'retval_'), 2)\n            File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_filevom8hoir.py\", line 40, in if_body_1\n                raise\n            File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_fileicn1_bl6.py\", line 41, in tf___embedding\n                seq_length = ag__.converted_call(ag__.ld(tf).shape, (ag__.ld(input_ids),), None, fscope)[1]\n        \n            ValueError: Exception encountered when calling layer 'embeddings' (type TFEmbeddings).\n            \n            in user code:\n            \n                File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_distilbert.py\", line 115, in call  *\n                    return self._embedding(inputs, training=training)\n                File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_distilbert.py\", line 139, in _embedding  *\n                    seq_length = tf.shape(input_ids)[1]\n            \n                ValueError: slice index 1 of dimension 0 out of bounds. for '{{node tf_distil_bert_model/distilbert/embeddings/strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](tf_distil_bert_model/distilbert/embeddings/Shape, tf_distil_bert_model/distilbert/embeddings/strided_slice/stack, tf_distil_bert_model/distilbert/embeddings/strided_slice/stack_1, tf_distil_bert_model/distilbert/embeddings/strided_slice/stack_2)' with input shapes: [1], [1], [1], [1] and with computed input tensors: input[1] = <1>, input[2] = <2>, input[3] = <1>.\n            \n            \n            Call arguments received by layer 'embeddings' (type TFEmbeddings):\n              • inputs=tf.Tensor(shape=(None,), dtype=string)\n              • mode=embedding\n              • training=False\n        \n        \n        Call arguments received by layer 'distilbert' (type TFDistilBertMainLayer):\n          • inputs=tf.Tensor(shape=(None,), dtype=string)\n          • attention_mask=None\n          • head_mask=None\n          • training=False\n    \n    \n    Call arguments received by layer 'tf_distil_bert_model' (type TFDistilBertModel):\n      • inputs=tf.Tensor(shape=(None,), dtype=string)\n      • kwargs={'training': 'False'}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rongg\\kuliah\\Semester 9\\NLP\\Praktikum\\1\\1.2.ipynb Cell 37\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rongg/kuliah/Semester%209/NLP/Praktikum/1/1.2.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m attention_model\u001b[39m.\u001b[39mpredict(encoded_input)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filept_egd4x.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filei_if7x8f.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m      9\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     10\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 11\u001b[0m outputs \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mdistilbert, (ag__\u001b[39m.\u001b[39mld(inputs),), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(kwargs)), fscope)\n\u001b[0;32m     12\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileexntdafw.py:83\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, inputs, attention_mask, head_mask, training)\u001b[0m\n\u001b[0;32m     81\u001b[0m     head_mask \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mnum_hidden_layers\n\u001b[0;32m     82\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mld(head_mask) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, if_body_3, else_body_3, get_state_3, set_state_3, (\u001b[39m'\u001b[39m\u001b[39mhead_mask\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39m1\u001b[39m)\n\u001b[1;32m---> 83\u001b[0m embedding_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39membeddings, (ag__\u001b[39m.\u001b[39mld(input_ids),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     84\u001b[0m tfmr_output \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mtransformer, ([ag__\u001b[39m.\u001b[39mld(embedding_output), ag__\u001b[39m.\u001b[39mld(attention_mask), ag__\u001b[39m.\u001b[39mld(head_mask)],), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(training)), fscope)\n\u001b[0;32m     85\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filevom8hoir.py:65\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, inputs, mode, training)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[39mraise\u001b[39;00m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mValueError\u001b[39;00m), (ag__\u001b[39m.\u001b[39mconverted_call(\u001b[39m'\u001b[39m\u001b[39mmode \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is not valid.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat, (ag__\u001b[39m.\u001b[39mld(mode),), \u001b[39mNone\u001b[39;00m, fscope),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     64\u001b[0m     ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mld(mode) \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m'\u001b[39m, if_body, else_body, get_state, set_state, (\u001b[39m'\u001b[39m\u001b[39mdo_return\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mretval_\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m2\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mld(mode) \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m'\u001b[39m, if_body_1, else_body_1, get_state_1, set_state_1, (\u001b[39m'\u001b[39m\u001b[39mdo_return\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mretval_\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m2\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[39mreturn\u001b[39;00m fscope\u001b[39m.\u001b[39mret(retval_, do_return)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filevom8hoir.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.if_body_1\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_embedding, (ag__\u001b[39m.\u001b[39mld(inputs),), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(training)), fscope)\n\u001b[0;32m     38\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileicn1_bl6.py:41\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___embedding\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m     39\u001b[0m input_ids \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     40\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mnot_(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39misinstance\u001b[39m), (ag__\u001b[39m.\u001b[39mld(inputs), (ag__\u001b[39m.\u001b[39mld(\u001b[39mtuple\u001b[39m), ag__\u001b[39m.\u001b[39mld(\u001b[39mlist\u001b[39m))), \u001b[39mNone\u001b[39;00m, fscope)), if_body, else_body, get_state, set_state, (\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mposition_ids\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m2\u001b[39m)\n\u001b[1;32m---> 41\u001b[0m seq_length \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mshape, (ag__\u001b[39m.\u001b[39mld(input_ids),), \u001b[39mNone\u001b[39;00m, fscope)[\u001b[39m1\u001b[39m]\n\u001b[0;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_1\u001b[39m():\n\u001b[0;32m     44\u001b[0m     \u001b[39mreturn\u001b[39;00m (position_ids,)\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2341, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2327, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2315, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2283, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_filei_if7x8f.py\", line 11, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).distilbert, (ag__.ld(inputs),), dict(**ag__.ld(kwargs)), fscope)\n    File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_fileexntdafw.py\", line 83, in tf__call\n        embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (ag__.ld(input_ids),), None, fscope)\n    File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_filevom8hoir.py\", line 65, in tf__call\n        ag__.if_stmt(ag__.ld(mode) == 'embedding', if_body_1, else_body_1, get_state_1, set_state_1, ('do_return', 'retval_'), 2)\n    File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_filevom8hoir.py\", line 40, in if_body_1\n        raise\n    File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_fileicn1_bl6.py\", line 41, in tf___embedding\n        seq_length = ag__.converted_call(ag__.ld(tf).shape, (ag__.ld(input_ids),), None, fscope)[1]\n\n    ValueError: Exception encountered when calling layer 'tf_distil_bert_model' (type TFDistilBertModel).\n    \n    in user code:\n    \n        File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_distilbert.py\", line 545, in call  *\n            outputs = self.distilbert(inputs, **kwargs)\n        File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_fileexntdafw.py\", line 83, in tf__call\n            embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (ag__.ld(input_ids),), None, fscope)\n        File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_filevom8hoir.py\", line 65, in tf__call\n            ag__.if_stmt(ag__.ld(mode) == 'embedding', if_body_1, else_body_1, get_state_1, set_state_1, ('do_return', 'retval_'), 2)\n        File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_filevom8hoir.py\", line 40, in if_body_1\n            raise\n        File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_fileicn1_bl6.py\", line 41, in tf___embedding\n            seq_length = ag__.converted_call(ag__.ld(tf).shape, (ag__.ld(input_ids),), None, fscope)[1]\n    \n        ValueError: Exception encountered when calling layer 'distilbert' (type TFDistilBertMainLayer).\n        \n        in user code:\n        \n            File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_distilbert.py\", line 435, in call  *\n                embedding_output = self.embeddings(input_ids)   # (bs, seq_length, dim)\n            File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_filevom8hoir.py\", line 65, in tf__call\n                ag__.if_stmt(ag__.ld(mode) == 'embedding', if_body_1, else_body_1, get_state_1, set_state_1, ('do_return', 'retval_'), 2)\n            File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_filevom8hoir.py\", line 40, in if_body_1\n                raise\n            File \"C:\\Users\\rongg\\AppData\\Local\\Temp\\__autograph_generated_fileicn1_bl6.py\", line 41, in tf___embedding\n                seq_length = ag__.converted_call(ag__.ld(tf).shape, (ag__.ld(input_ids),), None, fscope)[1]\n        \n            ValueError: Exception encountered when calling layer 'embeddings' (type TFEmbeddings).\n            \n            in user code:\n            \n                File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_distilbert.py\", line 115, in call  *\n                    return self._embedding(inputs, training=training)\n                File \"C:\\Users\\rongg\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_distilbert.py\", line 139, in _embedding  *\n                    seq_length = tf.shape(input_ids)[1]\n            \n                ValueError: slice index 1 of dimension 0 out of bounds. for '{{node tf_distil_bert_model/distilbert/embeddings/strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](tf_distil_bert_model/distilbert/embeddings/Shape, tf_distil_bert_model/distilbert/embeddings/strided_slice/stack, tf_distil_bert_model/distilbert/embeddings/strided_slice/stack_1, tf_distil_bert_model/distilbert/embeddings/strided_slice/stack_2)' with input shapes: [1], [1], [1], [1] and with computed input tensors: input[1] = <1>, input[2] = <2>, input[3] = <1>.\n            \n            \n            Call arguments received by layer 'embeddings' (type TFEmbeddings):\n              • inputs=tf.Tensor(shape=(None,), dtype=string)\n              • mode=embedding\n              • training=False\n        \n        \n        Call arguments received by layer 'distilbert' (type TFDistilBertMainLayer):\n          • inputs=tf.Tensor(shape=(None,), dtype=string)\n          • attention_mask=None\n          • head_mask=None\n          • training=False\n    \n    \n    Call arguments received by layer 'tf_distil_bert_model' (type TFDistilBertModel):\n      • inputs=tf.Tensor(shape=(None,), dtype=string)\n      • kwargs={'training': 'False'}\n"
     ]
    }
   ],
   "source": [
    "attention_model.predict(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "## Performance\n",
    "\n",
    "- LSTM MODEL\n",
    "\n",
    "    Training Accucacy : 0.71\n",
    "\n",
    "    Test Accucacy : 0.70\n",
    "\n",
    "- Word2Vec Embedding\n",
    "\n",
    "    Training Accucacy : 0.70\n",
    "\n",
    "    Test Accucacy : 0.64\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/distilbert-base-uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
